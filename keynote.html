<!DOCTYPE HTML>

<html>
	<head>
		<title>Welcome to KnowledgeNLP-ACL’24!</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">
					<!-- Logo -->
<!--						<img src="images/isail_logo.png" alt="" /><span><h2>Welcome to <a href="index.html" id="logo">iSAIL</a> Lab !</h2></span>-->

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="cfp.html">Calls</a></li>
								<li><a href="schedule.html">Schedule</a></li>
								<li class="current"><a href="keynote.html">Keynote</a></li>
								<li><a href="publications.html">Accepted Papers</a></li>
								<li><a href="organization.html">Organization</a></li>
							</ul>
						</nav>

				</div>
				<section id="banner">
					<!-- <header>
						<h2>Welcome to workshop on Knowledge Augmented Methods for NLP (KnowledgeNLP-ACL’24)!</h2>
						<a href="#" class="button">Learn More</a>
					</header> -->
				</section>



				<section class="wrapper style1">
					<div class="container">
						<div id="content">	
										

				</section>


	

				<section class="wrapper style1">
					<div class="container">
						<div id="content">	
					
					<div style="overflow: hidden;">
					   <div id="A" style="float:left; width: 25%;">
						  <img src="images/doug.png" alt="" width="220px" style="margin:0px 50px">
					   </div>
					   <div id="B" style="float: left; width: 75%;">
						  <div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							<b> <u>Prof. Doug Downey</u> </b><br>
						  </div>
						
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Directior at Allen Institute for AI and Professor Northwestern University<br>
							</div>

							<!-- <div>
								<b>Title: </b> The super alignment of of large language models <br>
								<b>Time: </b> 9:10 - 9:50 am <br>
								<b>Abstract: </b> With the rapid development of large language models such as ChatGPT and GPT-4, artificial general intelligence is getting closer and closer. But “the more intelligent AI is, the more dangerous it is.”, it becomes more and more crucial for us to make sure LLMs are really aligned and safe for our societies. In this talk, the speaker will talk about the  super alignment technologies to the super intelligence, including the alignment algorithms, training-free model extrapolation, and a conceptual framework for auto refinement. He will talk about weak-to-strong generalization, scalable oversight, auto red-teaming, exact optimization algorithm for alignment, and so on.  <br>
								<b>Bio: </b> Dr. Minlie Huang, professor of Tsinghua University,  deputy director of the Intelligent Technology and Systems Laboratory, deputy director of the Foundation Model Center of Tsinghua University. He was supported by National Distinguished Young Scholar project and NSFC key project. His research fields include large-scale language models, dialogue systems, and language generation. He authored a Chinese book "Modern Natural Language Generation". He published more than 150 papers in premier conferences and journals, with more than 20,000 Google Scholar citations, and is selected as Elsevier China's Highly Cited Scholars since 2022 and  the AI 2000 list of the world's most influential AI scholars since 2020; He has won several best papers or nominations at major international conferences (IJCAI, ACL, SIGDIAL, NLPCC, etc.). He led the development of several pretrained models including CDial-GPT, EVA, OPD, CPM, CharacterGLM. He serves as associate editors for TNNLS, TACL, CL, and TBD, and has served as the senior area chair of ACL/EMNLP/IJCAI/AAAI for more than 10 times. His homepage is located at http://coai.cs.tsinghua.edu.cn/hml/. <br>
							</div> -->
						</div>
					</div>
					<br>

					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/graham.jpg" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Prof. Graham Neubig </u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Associate Professor at the Carnegie Mellon University and Chief Scientist at All Hands AI<br>
							</div>
  
						  	<!-- <div>
								<b>Title: </b> Reliable, Adaptable and Attributable Language Models, Powered by Knowledge <br>
								<b>Time: </b> 9:50 - 10:30 am <br>
								<b>Abstract: </b> Large language models (LLMs), trained on extensive web data, demonstrate impressive flexibility and capability. Nevertheless, they continue to face practical challenges, including hallucinations, difficulties in adapting to new data distributions, and a lack of verifiability. Retrieval-augmented generation (RAG) has recently emerged as a popular approach to leveraging knowledge, which helps LLMs mitigate these weaknesses to some extent. However, RAG still falls short of providing a comprehensive solution to these problems. In this talk, I will discuss our recent efforts to enhance the factual accuracy and faithfulness of LLMs to the knowledge provided by retrieval. This includes an overview of how we can automatically evaluate the factuality of LLM responses and encourage LLMs to rely more heavily on explicit knowledge when generating responses. Furthermore, I will introduce a novel, efficient semi-parametric language model designed to reduce hallucinations and provide more attribution to their generations. Finally, I will conclude by discussing key open problems and outlining our planned approaches to addressing them in the near future. <br>
								<b>Bio: </b> Scott Wen-tau Yih is a Research Scientist at FAIR, Meta. His research interests include natural language processing, machine learning and information retrieval. Before joining Meta, Yih was a Principal Research Scientist at the Allen Institute for Artificial Intelligence (AI2), working on scientific question answering. Prior to that, Yih had spent 12 years at Microsoft Research, working on a variety of projects including email spam filtering, keyword extraction and search & ad relevance. His recent work focuses on continuous representations, neural models for question answering and retrieval, and retrieval-augmented language models; some of his well-known work includes WikiQA, RAG and DPR. Yih received the best paper award from CoNLL’11, an outstanding paper award from ACL’15 and has served as program co-chairs (CEAS’09, CoNLL’14, EMNLP’21) and senior area chairs for NLP (ACL, NNACL, EMNLP, EACL) and ML (ICLR, NeurIPS) conferences.   <br>
						  	</div> -->
						</div>
					</div>
					<br>



					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/yunyao.jpg" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Dr. Yunyao Li </u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Director of Machine Learning at Adobe Experience Platform<br>
							</div>
  
						  	<!-- <div>
								<b>Title: </b> Improving the Reliability of LLMs in Socially-Oriented Knowledge-Intensive Tasks <br>
								<b>Time: </b> 11:20 - 11:55 am <br>
								<b>Abstract: </b> LLMs excel at encoding real-world knowledge and using it for knowledge-intensive tasks. In socially-oriented domains, however, they often resort to hallucinations when the knowledge is implicit, subjective, or evolving. In this talk, I'll propose new directions to improve the reliability of LLMs in high-stakes, socially-oriented, knowledge-intensive tasks: (1) methods to abstain from generating a response when the LLM's internal knowledge is insufficient, and (2) methods to adapt the LLM's knowledge at decoding time and controllably steer it towards informative and more reliable outputs. I'll conclude with a discussion of future directions aimed at empowering LLMs to offer reliable responses on realistically complex, controversial, and socially-sensitive topics. <br>
								<b>Bio: </b> Yulia Tsvetkov is an associate professor at the Paul G. Allen School of Computer Science & Engineering at University of Washington. Her research group works on fundamental advancements to large language models, multilingual NLP, and AI ethics. This research is motivated by a unified goal: to extend the capabilities of human language technology beyond individual populations and across language boundaries, thereby making NLP tools available to all users. Prior to joining UW, Yulia was an assistant professor at Carnegie Mellon University and a postdoc at Stanford. Yulia is a recipient of NSF CAREER, Sloan Fellowship, Okawa Research award, and several paper awards and runner-ups at NLP, ML, and CSS conferences. <br>
						  	</div> -->
						</div>
					</div>
					<br>




					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/yoav.jpg" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Prof. Yoav Artzi</u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Associate Professor at Cornell Tech<br>
							</div>
  
						  	<!-- <div>
								<b>Title: </b> Improving the Reliability of LLMs in Socially-Oriented Knowledge-Intensive Tasks <br>
								<b>Time: </b> 11:20 - 11:55 am <br>
								<b>Abstract: </b> LLMs excel at encoding real-world knowledge and using it for knowledge-intensive tasks. In socially-oriented domains, however, they often resort to hallucinations when the knowledge is implicit, subjective, or evolving. In this talk, I'll propose new directions to improve the reliability of LLMs in high-stakes, socially-oriented, knowledge-intensive tasks: (1) methods to abstain from generating a response when the LLM's internal knowledge is insufficient, and (2) methods to adapt the LLM's knowledge at decoding time and controllably steer it towards informative and more reliable outputs. I'll conclude with a discussion of future directions aimed at empowering LLMs to offer reliable responses on realistically complex, controversial, and socially-sensitive topics. <br>
								<b>Bio: </b> Yulia Tsvetkov is an associate professor at the Paul G. Allen School of Computer Science & Engineering at University of Washington. Her research group works on fundamental advancements to large language models, multilingual NLP, and AI ethics. This research is motivated by a unified goal: to extend the capabilities of human language technology beyond individual populations and across language boundaries, thereby making NLP tools available to all users. Prior to joining UW, Yulia was an assistant professor at Carnegie Mellon University and a postdoc at Stanford. Yulia is a recipient of NSF CAREER, Sloan Fellowship, Okawa Research award, and several paper awards and runner-ups at NLP, ML, and CSS conferences. <br>
						  	</div> -->
						</div>
					</div>
					<br>




					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/manling.jpg" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Prof. Manling Li</u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Assistant Professor at Northwestern University <br>
							</div>
  
						  	<!-- <div>
								<b>Title: </b> Improving the Reliability of LLMs in Socially-Oriented Knowledge-Intensive Tasks <br>
								<b>Time: </b> 11:20 - 11:55 am <br>
								<b>Abstract: </b> LLMs excel at encoding real-world knowledge and using it for knowledge-intensive tasks. In socially-oriented domains, however, they often resort to hallucinations when the knowledge is implicit, subjective, or evolving. In this talk, I'll propose new directions to improve the reliability of LLMs in high-stakes, socially-oriented, knowledge-intensive tasks: (1) methods to abstain from generating a response when the LLM's internal knowledge is insufficient, and (2) methods to adapt the LLM's knowledge at decoding time and controllably steer it towards informative and more reliable outputs. I'll conclude with a discussion of future directions aimed at empowering LLMs to offer reliable responses on realistically complex, controversial, and socially-sensitive topics. <br>
								<b>Bio: </b> Yulia Tsvetkov is an associate professor at the Paul G. Allen School of Computer Science & Engineering at University of Washington. Her research group works on fundamental advancements to large language models, multilingual NLP, and AI ethics. This research is motivated by a unified goal: to extend the capabilities of human language technology beyond individual populations and across language boundaries, thereby making NLP tools available to all users. Prior to joining UW, Yulia was an assistant professor at Carnegie Mellon University and a postdoc at Stanford. Yulia is a recipient of NSF CAREER, Sloan Fellowship, Okawa Research award, and several paper awards and runner-ups at NLP, ML, and CSS conferences. <br>
						  	</div> -->
						</div>
					</div>
					<br>



					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/hongming.png" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Dr. Hongming Zhang</u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Senior Research Scientist at Tencent AI Lab<br>
							</div>
  
						  	<!-- <div>
								<b>Title: </b> Improving the Reliability of LLMs in Socially-Oriented Knowledge-Intensive Tasks <br>
								<b>Time: </b> 11:20 - 11:55 am <br>
								<b>Abstract: </b> LLMs excel at encoding real-world knowledge and using it for knowledge-intensive tasks. In socially-oriented domains, however, they often resort to hallucinations when the knowledge is implicit, subjective, or evolving. In this talk, I'll propose new directions to improve the reliability of LLMs in high-stakes, socially-oriented, knowledge-intensive tasks: (1) methods to abstain from generating a response when the LLM's internal knowledge is insufficient, and (2) methods to adapt the LLM's knowledge at decoding time and controllably steer it towards informative and more reliable outputs. I'll conclude with a discussion of future directions aimed at empowering LLMs to offer reliable responses on realistically complex, controversial, and socially-sensitive topics. <br>
								<b>Bio: </b> Yulia Tsvetkov is an associate professor at the Paul G. Allen School of Computer Science & Engineering at University of Washington. Her research group works on fundamental advancements to large language models, multilingual NLP, and AI ethics. This research is motivated by a unified goal: to extend the capabilities of human language technology beyond individual populations and across language boundaries, thereby making NLP tools available to all users. Prior to joining UW, Yulia was an assistant professor at Carnegie Mellon University and a postdoc at Stanford. Yulia is a recipient of NSF CAREER, Sloan Fellowship, Okawa Research award, and several paper awards and runner-ups at NLP, ML, and CSS conferences. <br>
						  	</div> -->
						</div>
					</div>
					<br>



				</section>

					<!-- Copyright -->
						<div class="copyright">
							<ul class="menu">
								<li>&copy;  All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>

				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
